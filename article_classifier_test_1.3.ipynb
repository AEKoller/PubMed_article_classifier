{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running integration test with query: 'Assisted+dying', max articles: 5, model: llama3\n",
      "Total articles found: 3878\n",
      "Fetching up to 5 articles...\n",
      "Successfully fetched 5 articles.\n",
      "After preprocessing, 4 articles remain.\n",
      "\n",
      "Processing article 1 of 4...\n",
      "Article ID: 39160544\n",
      "DOI: 10.1186/s12904-024-01542-z\n",
      "Title: Non-invasive technology to assess hydration status in advanced cancer to explore relationships between fluid status and symptoms: an observational study using bioelectrical impedance analysis.\n",
      "Study Type: ** Empirical\n",
      "Research Methods: ** BIA, observational study\n",
      "\n",
      "Processing article 2 of 4...\n",
      "Article ID: 39157533\n",
      "DOI: 10.3389/fpubh.2024.1399025\n",
      "Title: Readiness of nurses when faced with a patient's death.\n",
      "Study Type: ** Empirical\n",
      "Research Methods: ** Surveys, interviews\n",
      "\n",
      "Processing article 3 of 4...\n",
      "Article ID: 39157418\n",
      "DOI: 10.1177/26323524241272102\n",
      "Title: 'There is no such word as palliative care for us at the moment': A mixed-method study exploring the perceptions of healthcare professionals on the need for palliative care in Bhutan.\n",
      "Study Type: ** Empirical\n",
      "Research Methods: ** Mixed-methods study, semi-structured interviews, qualitative analysis\n",
      "\n",
      "Processing article 4 of 4...\n",
      "Article ID: 39152645\n",
      "DOI: 10.1177/02692163241269689\n",
      "Title: The double awareness of the wish to hasten death and the will to live: A secondary analysis of outlier patients from a mixed-methods study.\n",
      "Study Type: ** Empirical\n",
      "Research Methods: ** Mixed-methods study, secondary analysis of outlier patients\n",
      "\n",
      "Validation complete. 4 out of 4 results passed validation.\n",
      "\n",
      "Integration test complete!\n",
      "Results saved to pubmed_ollama_test_results_20240821_192634.json\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 201\u001b[0m\n\u001b[1;32m    198\u001b[0m base_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpubmed_ollama_test_results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m results \u001b[38;5;241m=\u001b[39m run_integration_test()\n\u001b[0;32m--> 201\u001b[0m \u001b[43msave_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 156\u001b[0m, in \u001b[0;36msave_results\u001b[0;34m(results, base_filename)\u001b[0m\n\u001b[1;32m    154\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriteheader()\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m--> 156\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/myenv/lib/python3.9/csv.py:154\u001b[0m, in \u001b[0;36mDictWriter.writerow\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriterow\u001b[39m(\u001b[38;5;28mself\u001b[39m, rowdict):\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mwriterow(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dict_to_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowdict\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/myenv/lib/python3.9/csv.py:149\u001b[0m, in \u001b[0;36mDictWriter._dict_to_list\u001b[0;34m(self, rowdict)\u001b[0m\n\u001b[1;32m    147\u001b[0m     wrong_fields \u001b[38;5;241m=\u001b[39m rowdict\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrong_fields:\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdict contains fields not in fieldnames: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m                          \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m wrong_fields]))\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (rowdict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestval) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfieldnames)\n",
      "\u001b[0;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'id'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "def fetch_pubmed_articles(query, max_results=5):\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "    \n",
    "    search_url = f\"{base_url}esearch.fcgi?db=pubmed&term={query}&usehistory=y&retmode=json\"\n",
    "    search_response = requests.get(search_url).json()\n",
    "    \n",
    "    total_count = int(search_response['esearchresult']['count'])\n",
    "    webenv = search_response['esearchresult']['webenv']\n",
    "    query_key = search_response['esearchresult']['querykey']\n",
    "    \n",
    "    print(f\"Total articles found: {total_count}\")\n",
    "    print(f\"Fetching up to {max_results} articles...\")\n",
    "\n",
    "    fetch_url = f\"{base_url}efetch.fcgi?db=pubmed&query_key={query_key}&WebEnv={webenv}&retmode=xml&retmax={max_results}\"\n",
    "    fetch_response = requests.get(fetch_url)\n",
    "    \n",
    "    root = ET.fromstring(fetch_response.content)\n",
    "    \n",
    "    articles = []\n",
    "    for article in root.findall(\".//PubmedArticle\"):\n",
    "        pmid = article.find(\".//PMID\").text if article.find(\".//PMID\") is not None else \"ID not available\"\n",
    "        title_element = article.find(\".//ArticleTitle\")\n",
    "        title = title_element.text if title_element is not None else \"Title not available\"\n",
    "        abstract_element = article.find(\".//Abstract/AbstractText\")\n",
    "        abstract = abstract_element.text if abstract_element is not None else \"Abstract not available\"\n",
    "        doi_element = article.find(\".//ArticleId[@IdType='doi']\")\n",
    "        doi = doi_element.text if doi_element is not None else \"DOI not available\"\n",
    "        \n",
    "        articles.append({\n",
    "            'id': pmid,\n",
    "            'doi': doi,\n",
    "            'title': title,\n",
    "            'abstract': abstract\n",
    "        })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def preprocess_article(article):\n",
    "    # Check if the article is None\n",
    "    if article is None:\n",
    "        return None\n",
    "\n",
    "    # Check if 'abstract' key exists and is not None\n",
    "    if 'abstract' not in article or article['abstract'] is None:\n",
    "        return None\n",
    "\n",
    "    # Check if the abstract is too short or not available\n",
    "    if len(article['abstract']) < 50 or article['abstract'] == \"Abstract not available.\":\n",
    "        return None\n",
    "    \n",
    "    # Check if 'title' key exists and is not None\n",
    "    if 'title' not in article or article['title'] is None:\n",
    "        return None\n",
    "\n",
    "    # Check if the title is missing\n",
    "    if article['title'] == \"Title not available.\":\n",
    "        return None\n",
    "    \n",
    "    return article\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))\n",
    "def process_article_with_ollama(article, model_name=\"llama3\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following research article on assisted dying:\n",
    "\n",
    "    Title: {article['title']}\n",
    "\n",
    "    Abstract: {article['abstract']}\n",
    "\n",
    "    Please provide a detailed analysis addressing the following points. Format your response exactly as shown below:\n",
    "\n",
    "    Study Type: [Empirical/Theoretical]\n",
    "\n",
    "    Study Type Justification: [Your explanation here]\n",
    "\n",
    "    Research Methods: [List the specific methods used, separated by commas]\n",
    "\n",
    "    Research Methods Justification: [Your explanation here]\n",
    "\n",
    "    Ensure your response follows this exact format for easy parsing.\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()['response']\n",
    "        return parse_llm_response(result)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error processing article {article['id']}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def parse_llm_response(response):\n",
    "    parsed_result = {\n",
    "        'study_type': \"Unable to determine\",\n",
    "        'study_type_justification': \"Unable to determine\",\n",
    "        'research_methods': \"Unable to determine\",\n",
    "        'research_methods_justification': \"Unable to determine\"\n",
    "    }\n",
    "    \n",
    "    # Use more flexible regex patterns\n",
    "    patterns = {\n",
    "        'study_type': r\"Study Type:\\s*(.+?)(?:\\n|$)\",\n",
    "        'study_type_justification': r\"Study Type Justification:\\s*(.+?)(?:\\n\\n|\\n[A-Z]|$)\",\n",
    "        'research_methods': r\"Research Methods:\\s*(.+?)(?:\\n|$)\",\n",
    "        'research_methods_justification': r\"Research Methods Justification:\\s*(.+?)(?:\\n\\n|$)\"\n",
    "    }\n",
    "    \n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            parsed_result[key] = match.group(1).strip()\n",
    "    \n",
    "    return parsed_result\n",
    "\n",
    "def validate_results(results):\n",
    "    validated_results = []\n",
    "    for result in results:\n",
    "        if all(value != \"Unable to determine\" for value in result.values()):\n",
    "            validated_results.append(result)\n",
    "        else:\n",
    "            print(f\"Incomplete result for article {result['article_id']}. Skipping.\")\n",
    "    return validated_results\n",
    "\n",
    "def save_results(results, base_filename):\n",
    "    # Save as JSON\n",
    "    json_filename = f\"{base_filename}.json\"\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"Results saved to {json_filename}\")\n",
    "\n",
    "    # Save as CSV\n",
    "    csv_filename = f\"{base_filename}.csv\"\n",
    "    \n",
    "    # Get all unique keys from all result dictionaries\n",
    "    fieldnames = set()\n",
    "    for result in results:\n",
    "        fieldnames.update(result.keys())\n",
    "    fieldnames = sorted(list(fieldnames))  # Sort field names for consistency\n",
    "\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for result in results:\n",
    "            writer.writerow(result)\n",
    "    print(f\"Results saved to {csv_filename}\")\n",
    "\n",
    "def run_integration_test(query=\"Assisted+dying\", max_articles=5, model_name=\"llama3\"):\n",
    "    print(f\"Running integration test with query: '{query}', max articles: {max_articles}, model: {model_name}\")\n",
    "    \n",
    "    # Fetch articles\n",
    "    articles = fetch_pubmed_articles(query, max_articles)\n",
    "    print(f\"Successfully fetched {len(articles)} articles.\")\n",
    "    \n",
    "    # Preprocess and filter articles\n",
    "    preprocessed_articles = [preprocess_article(article) for article in articles]\n",
    "    preprocessed_articles = [article for article in preprocessed_articles if article is not None]\n",
    "    print(f\"After preprocessing, {len(preprocessed_articles)} articles remain.\")\n",
    "    \n",
    "    # Process articles with Ollama\n",
    "    results = []\n",
    "    for i, article in enumerate(preprocessed_articles, 1):\n",
    "        print(f\"\\nProcessing article {i} of {len(preprocessed_articles)}...\")\n",
    "        try:\n",
    "            llm_result = process_article_with_ollama(article, model_name)\n",
    "            result = {**article, **llm_result}\n",
    "            results.append(result)\n",
    "            print(f\"Article ID: {result['id']}\")\n",
    "            print(f\"DOI: {result['doi']}\")\n",
    "            print(f\"Title: {result['title']}\")\n",
    "            print(f\"Study Type: {result['study_type']}\")\n",
    "            print(f\"Research Methods: {result['research_methods']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process article {article['id']}: {str(e)}\")\n",
    "        time.sleep(2)  # Increased delay to be more respectful to the Ollama API\n",
    "    \n",
    "    # Validate results\n",
    "    validated_results = validate_results(results)\n",
    "    print(f\"\\nValidation complete. {len(validated_results)} out of {len(results)} results passed validation.\")\n",
    "    \n",
    "    print(\"\\nIntegration test complete!\")\n",
    "    return validated_results\n",
    "\n",
    "# Run the integration test\n",
    "if __name__ == \"__main__\":\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base_filename = f\"pubmed_ollama_test_results_{timestamp}\"\n",
    "    \n",
    "    results = run_integration_test()\n",
    "    save_results(results, base_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
